{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f9fedfdd-a7d9-47d2-84d7-9a47c512cab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "\n",
    "bucket_name = \"bigdata-chmurki-nifi\"\n",
    "client = storage.Client()\n",
    "\n",
    "def modify_name(value):\n",
    "    if str(value).startswith('N'):\n",
    "        return '99' + str(value)[1:].zfill(2)\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "\n",
    "def minutes_to_midnight(time_value):\n",
    "    current_time = datetime.combine(datetime.today(), time_value)\n",
    "    midnight = datetime.combine(datetime.today(), datetime.min.time()) + timedelta(days=1)\n",
    "    minutes_to_next_midnight = (midnight - current_time).seconds // 60\n",
    "    minutes_from_last_midnight = current_time.hour * 60 + current_time.minute\n",
    "    return min(minutes_to_next_midnight, minutes_from_last_midnight)\n",
    "\n",
    "def read_positions(date = None):\n",
    "    '''\n",
    "    Function to read vehicles positions. If date is specified, it reads data only from given date.\n",
    "    Else, it reads all the data found (e.g. for model first training).\n",
    "    '''\n",
    "    positions = []\n",
    "    \n",
    "    for blob in client.list_blobs(bucket_name, prefix='warsaw'):\n",
    "        if date is None:\n",
    "            if blob.name.count('Act_position') == 1:\n",
    "                table = pq.read_table(io.BytesIO(blob.download_as_bytes()))\n",
    "                df = table.to_pandas()\n",
    "                df = df.explode('vehicles').reset_index(drop=True)\n",
    "                df_flat = pd.json_normalize(df['vehicles'])\n",
    "                df_flat = pd.concat([df_flat, df], axis=1)\n",
    "                positions.append(df_flat)\n",
    "        else:\n",
    "            if blob.name.count(f'Act_position_{date}') == 1:\n",
    "                table = pq.read_table(io.BytesIO(blob.download_as_bytes()))\n",
    "                df = table.to_pandas()\n",
    "                df = df.explode('vehicles').reset_index(drop=True)\n",
    "                df_flat = pd.json_normalize(df['vehicles'])\n",
    "                df_flat = pd.concat([df_flat, df], axis=1)\n",
    "                positions.append(df_flat)\n",
    "        \n",
    "    positions_df = pd.concat(positions, ignore_index=True)\n",
    "    \n",
    "    cols_to_drop = ['vehicles',\n",
    "            'headsign',\n",
    "            'vehicleCode',\n",
    "            'vehicleId',\n",
    "            'direction',\n",
    "            'gpsQuality',\n",
    "            'tripId',\n",
    "            'tripId.member0',\n",
    "            'tripId.member1',\n",
    "            'lastUpdate']\n",
    "    for col in cols_to_drop:\n",
    "        try:\n",
    "            positions_df = positions_df.drop(col,axis=1)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    positions_df = positions_df.drop_duplicates().reset_index(drop=True)\n",
    "    positions_df['routeShortName'] = positions_df['routeShortName'].apply(lambda x: modify_name(x))\n",
    "    return positions_df\n",
    "\n",
    "def add_15_minutes(positions_df):\n",
    "    '''\n",
    "    Function that based on given vehicles positions dataframes reads their delay in 15 minutes.\n",
    "    '''\n",
    "    df = positions_df.copy()\n",
    "    df['generated'] = pd.to_datetime(df['generated'])\n",
    "    df['generated_weather_bin'] = df['generated'].dt.round('10min')\n",
    "    df = df.dropna()\n",
    "    df['time'] = df['generated'].apply(lambda x: x.time())\n",
    "    df['generated'] = df['generated'].dt.round('1min')\n",
    "    df.drop_duplicates(subset=['generated', 'vehicleService', 'scheduledTripStartTime'], keep='first', inplace=True)\n",
    "\n",
    "    df['generated_15min'] = df['generated'] + pd.to_timedelta('15min')\n",
    "    merged_df = pd.merge(df, df, left_on=['vehicleService', 'scheduledTripStartTime', 'generated_15min'],\n",
    "                              right_on=['vehicleService', 'scheduledTripStartTime', 'generated'],\n",
    "                              suffixes=('', '_15min'))\n",
    "    merged_df = merged_df.drop(['generated_15min','generated_15min',\n",
    "                               'routeShortName_15min','speed_15min',\n",
    "                                'generated_15min_15min','generated_weather_bin_15min'],axis=1)\n",
    "    \n",
    "    merged_df['time'] = merged_df['time'].apply(lambda x: minutes_to_midnight(x))\n",
    "    merged_df['time_15min'] = merged_df['time_15min'].apply(lambda x: minutes_to_midnight(x))\n",
    "    return merged_df\n",
    "\n",
    "def read_weather(date=None):\n",
    "    actuals = []\n",
    "    forecasts = []\n",
    "    \n",
    "    for blob in client.list_blobs(bucket_name, prefix='weather'):\n",
    "        if date is None:\n",
    "            if blob.name.count('Act_weather_') == 1:\n",
    "                table = pq.read_table(io.BytesIO(blob.download_as_bytes()))\n",
    "                df = table.to_pandas()\n",
    "                df_flat = pd.json_normalize(df['current'])\n",
    "                df_flat = pd.concat([df_flat, df], axis=1)\n",
    "                df_flat = df_flat.drop(['current'],axis=1)\n",
    "                actuals.append(df_flat)\n",
    "            elif blob.name.count('FC_weather_') == 1:\n",
    "                table = pq.read_table(io.BytesIO(blob.download_as_bytes()))\n",
    "                df = table.to_pandas()\n",
    "                df_flat = pd.json_normalize(df['hourly'])\n",
    "                df_flat['data'] = df_flat['data'].apply(lambda x: x[0] if x is not None and len(x) > 0 else None)\n",
    "                df_flat = pd.json_normalize(df_flat['data'])\n",
    "                df_flat = pd.concat([df_flat, df], axis=1)\n",
    "                df_flat = df_flat.drop(['hourly'],axis=1)\n",
    "                forecasts.append(df_flat)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if blob.name.count(f'Act_weather_{date}') == 1:\n",
    "                table = pq.read_table(io.BytesIO(blob.download_as_bytes()))\n",
    "                df = table.to_pandas()\n",
    "                df_flat = pd.json_normalize(df['current'])\n",
    "                df_flat = pd.concat([df_flat, df], axis=1)\n",
    "                df_flat = df_flat.drop(['current'],axis=1)\n",
    "                actuals.append(df_flat)\n",
    "            elif blob.name.count('FC_weather_') == 1:\n",
    "                table = pq.read_table(io.BytesIO(blob.download_as_bytes()))\n",
    "                df = table.to_pandas()\n",
    "                df_flat = pd.json_normalize(df['hourly'])\n",
    "                df_flat['data'] = df_flat['data'].apply(lambda x: x[0] if x is not None and len(x) > 0 else None)\n",
    "                df_flat = pd.json_normalize(df_flat['data'])\n",
    "                df_flat = pd.concat([df_flat, df], axis=1)\n",
    "                df_flat = df_flat.drop(['hourly'],axis=1)\n",
    "                forecasts.append(df_flat)\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "    actual_df = pd.concat(actuals, ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "    forecast_df = pd.concat(forecasts, ignore_index=True).drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    actual_df['time'] = pd.to_datetime(actual_df['time'])\n",
    "    actual_df['generated_weather_bin'] = actual_df['time'].dt.round('10min')\n",
    "    actual_df['generated_forecast_bin'] = actual_df['time'].dt.round('60min').dt.tz_convert('UTC')+ pd.Timedelta(hours=1)\n",
    "    \n",
    "    actual_df = actual_df.drop(['time','icon','icon_num','wind.dir',\n",
    "                                'lat','lon','elevation','timezone','units',\n",
    "                                'hourly','daily'],axis=1)\n",
    "    \n",
    "    forecast_df['date'] = pd.to_datetime(forecast_df['date']).dt.tz_localize('UTC')\n",
    "    forecast_df.rename(columns={'cloud_cover.total': 'cloud_cover'}, inplace=True)\n",
    "    forecast_df = forecast_df.drop(['weather','icon','wind.dir',\n",
    "                                    'lat','lon','elevation','timezone','units','current','daily','time'],axis=1)\n",
    "    \n",
    "    weather_df = pd.merge(actual_df,forecast_df,left_on = 'generated_forecast_bin',right_on='date',\n",
    "                          suffixes=('_actual', '_forecast'))\n",
    "    weather_df = weather_df.drop(['generated_forecast_bin','date'],axis=1)\n",
    "    weather_df = weather_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    return weather_df\n",
    "    \n",
    "\n",
    "def finish_df(df):\n",
    "    df = df.drop(['generated','generated_weather_bin','vehicleService', 'scheduledTripStartTime'],axis=1)\n",
    "    cols_to_ohe = ['summary_actual', 'precipitation.type_actual', 'summary_forecast', 'precipitation.type_forecast']\n",
    "    ohes = []\n",
    "\n",
    "    for col in cols_to_ohe:\n",
    "        ohes.append(pd.get_dummies(df[col], prefix=col))\n",
    "\n",
    "    df = pd.concat([df]+ohes, axis=1)\n",
    "    df = df.drop(cols_to_ohe, axis=1)\n",
    "    return df\n",
    "\n",
    "def read_timetable(date = None):\n",
    "    '''\n",
    "    Function that reads timetable for all the trips scheduled for given day (or all day if date is None).\n",
    "    '''\n",
    "    timetables = []\n",
    "    \n",
    "    for blob in client.list_blobs(bucket_name, prefix='warsaw'):\n",
    "        if date is not None:\n",
    "            if blob.name.count(f'Stops_{date}') == 1:\n",
    "                timetable = pd.read_csv(f'gs://{bucket_name}/{blob.name}')\n",
    "                timetables.append(timetable)\n",
    "                break\n",
    "        else:\n",
    "            if blob.name.count(f'Stops') == 1:\n",
    "                timetable = pd.read_csv(f'gs://{bucket_name}/{blob.name}')\n",
    "                timetables.append(timetable)\n",
    "                \n",
    "    timetable_df = pd.concat(timetables, ignore_index=True)\n",
    "    timetable_df = timetable_df[['trip_id','arrival_time','stop_id','stop_sequence']]\n",
    "    \n",
    "    for blob in client.list_blobs(bucket_name, prefix='warsaw'):\n",
    "        if date is None:\n",
    "            date = '2023-12-17'\n",
    "        if blob.name.count(f'Stop_times_position_{date}') == 1:\n",
    "            stops = pd.read_csv(f'gs://{bucket_name}/{blob.name}')\n",
    "            break\n",
    "    stops = stops[['stop_id','stop_lat','stop_lon']]\n",
    "    \n",
    "    timetable_full = pd.merge(timetable_df,stops,on='stop_id')\n",
    "    timetable_full = timetable_full.drop_duplicates().reset_index(drop=True)\n",
    "    return timetable_full\n",
    "\n",
    "def find_future_coords(df, datetime_str, code, delay, time):\n",
    "    '''\n",
    "    Function that for given vehicleService and scheduledTrupStartTime (composite identifier of trip),\n",
    "    current delay and time returns position of expected stop in 15 minutes.\n",
    "    '''\n",
    "    dt_components = pd.to_datetime(datetime_str)\n",
    "    year = str(dt_components.year)\n",
    "    month = str(dt_components.month).zfill(2)  \n",
    "    day = str(dt_components.day).zfill(2)\n",
    "    hour = str(dt_components.hour).zfill(2)\n",
    "    minute = str(dt_components.minute).zfill(2)\n",
    "    pattern = f'{code.split(\"-\")[0]}{year}{month}{day}{hour}{minute}.*_{code}'\n",
    "    time_format = \"%H:%M:%S\"\n",
    "    filtered_df = df[df['trip_id'].str.contains(pattern, case=False)].sort_values(['arrival_time']).reset_index(drop=True)\n",
    "    filtered_df = filtered_df[filtered_df['arrival_time']>=(datetime.strptime(time, time_format) - timedelta(minutes=int(delay))).strftime(time_format)].head(1)\n",
    "    \n",
    "    return filtered_df['stop_lat'], filtered_df['stop_lon']\n",
    "\n",
    "def add_future_coords(df, timetable):\n",
    "    '''\n",
    "    Function that appends columns with predicted coords to each row according to timetable.\n",
    "    '''\n",
    "    def calculate_future_time(generated_time):\n",
    "        future_time = generated_time + pd.to_timedelta('15min')\n",
    "        return future_time.strftime('%H:%M:%S')\n",
    "\n",
    "    df['future_time'] = df['generated'].apply(calculate_future_time)\n",
    "    df['coords'] = df.apply(lambda row: find_future_coords(timetable, row['scheduledTripStartTime'], row['vehicleService'], row['delay'], row['future_time']), axis=1)\n",
    "    df['lat_15min'] = df['coords'].apply(lambda x: x[0])\n",
    "    df['lon_15min'] = df['coords'].apply(lambda x: x[1])\n",
    "    df.drop(['coords','future_time'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f214b4b-f4a2-45fd-94bf-e9c078121fe2",
   "metadata": {},
   "source": [
    "## Example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9e054060-d83e-4ef4-b806-e335c08a1e67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = read_positions()\n",
    "df = add_15_minutes(df)\n",
    "w = read_weather()\n",
    "df = pd.merge(df,w,on='generated_weather_bin')\n",
    "df = finish_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5a4468ca-a487-49e2-ba57-51df175f4a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5038159983319095"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, df.columns != 'delay_15min'], df[\"delay_15min\"], test_size=0.33, random_state=42)\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=100, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7bb9f70a-f8d3-4936-a408-9869d25d1145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open(\"model.sav\", 'wb'))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
